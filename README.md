#**Behavioral Cloning** 

##Writeup Template

###You can use this file as a template for your writeup if you want to submit it as a markdown file, but feel free to use some other method and submit a pdf if you prefer.

---

**Behavioral Cloning Project**

The goals / steps of this project are the following:
* Use the simulator to collect data of good driving behavior
* Build, a convolution neural network in Keras that predicts steering angles from images
* Train and validate the model with a training and validation set
* Test that the model successfully drives around track one without leaving the road
* Summarize the results with a written report


[//]: # (Image References)

[image1]: ./examples/placeholder.png "Model Visualization"
[image2]: ./examples/placeholder.png "Grayscaling"
[image3]: ./examples/placeholder_small.png "Recovery Image"
[image4]: ./examples/placeholder_small.png "Recovery Image"
[image5]: ./examples/placeholder_small.png "Recovery Image"
[image6]: ./examples/placeholder_small.png "Normal Image"
[image7]: ./examples/placeholder_small.png "Flipped Image"

## Rubric Points
###Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/432/view) individually and describe how I addressed each point in my implementation.  

---
###Files Submitted & Code Quality

####1. Submission includes all required files and can be used to run the simulator in autonomous mode

My project includes the following files:
* model.py containing the script to create and train the model
* drive.py for driving the car in autonomous mode
* model.h5 containing a trained convolution neural network 
* writeup_report.md or writeup_report.pdf summarizing the results

####2. Submission includes functional code
Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing 
```
python drive.py model.json
```

####3. Submission code is usable and readable

The P3.ipynb file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works.

###Model Architecture and Training Strategy

####1. An appropriate model architecture has been employed. The model uses lambda layer at fist to normalize ,then 8 convolutional layers, followed by a dropout layer. Then it uses 2 dense layers and another dropout layer for angle prediction. The model uses ELU to introduce nonlinearity. 

####2. Attempts to reduce overfitting in the model

The model contains dropout layers in order to reduce overfitting with keep_prob = 0.2. 

The model was trained and validated on different data sets to ensure that the model was not overfitting. The model was tested by running it through the simulator and ensuring that the vehicle could stay on the track.

####3. Model parameter tuning

The model used an adam optimizer, with default learning rate of 0.001.

####4. Appropriate training data

Training data was chosen to keep the vehicle driving on the road. I used a combination of center lane driving, recovering from the left and right sides of the road to ensure this. Futher explanation has been given in the first section of the code (cell 1)

 

###Model Architecture and Training Strategy

####1. Solution Design Approach

The overall strategy for deriving a model architecture was to keep on experimenting with the convolutional layers, filter size and no. of dense layers and no. of neurons in dense layers. I also followed the designs defined in comma.ai for same purpose.

I initially used the convolutional neural network, I used for traffic sign recognition problem, after re-creating it in keras. But it produced very poor results. Hence I started increasing the convolutional layers and gradually the filter size as well.  

In order to gauge how well the model was working, I split my image and steering angle data into a training and validation set. I found that my first model had a moderately low mean squared error on the training set but a slighly higher mean squared error on the validation set. The model was barely able to keep the vehicle in the track whereever the track was straight. But it was failing terribly near the turns. So, I had to include data in training dataset so that the model learns to do recovery, in case vehicle reaches the edges of the track. 

At this point I decided to include more images in the training dataset. I tried generating more data by using the recording feature in simulator. But steering angles generated by keyboard was not smooth at all, it was rather steep. I figured this is not going to help the model to learn. So, I decided not to use this data and rather turned to the images captured from right and left camera. I synthetically modified the steering angles, as described in the code. (cell 1 in P3.ipynb)
I also flipped the images and reversed the steering angles to generate more data.

Also, I checked the images provided by Udacity and observed in some of the images , vehicle was not kept at the center of the track. I, therefore, decided to remove these data from training dataset.

During the training I used kera image generator also to make the training data more robust.



To combat the overfitting, I included dropout layers in the model and also removed some of the images, where steering angle is zero. This helped me to prevent the model from overfitting to zero angle.

I also tested the model on the same images, it was trained on and plotted the predicted angle and actual angle against each other to see the overall picture. I was expecting to see a pattern of 45 degree angle in the plot, as it'll give a glimpse of how well the model can do, even before testing it out in the track.

I kept of working with this cycle and finally was able to narrow down on a model, which was able to drive the car around the first track decently and yet it did not have a huge no. of convolutional layers like VGG, as I had limited computional resources. 


At the end of the process, the vehicle is able to drive autonomously around the track without leaving the road.

####2. Final Model Architecture

Following is the summary of the model taken from keras.

Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
lambda_3 (Lambda)                (None, 40, 80, 3)     0           lambda_input_3[0][0]             
____________________________________________________________________________________________________
conv1 (Convolution2D)            (None, 14, 27, 8)     1184        lambda_3[0][0]                   
____________________________________________________________________________________________________
conv1_1 (Convolution2D)          (None, 5, 9, 8)       3144        conv1[0][0]                      
____________________________________________________________________________________________________
conv4 (Convolution2D)            (None, 2, 3, 64)      12864       conv1_1[0][0]                    
____________________________________________________________________________________________________
conv4_4 (Convolution2D)          (None, 1, 1, 64)      102464      conv4[0][0]                      
____________________________________________________________________________________________________
conv5 (Convolution2D)            (None, 1, 1, 128)     131200      conv4_4[0][0]                    
____________________________________________________________________________________________________
conv5_5 (Convolution2D)          (None, 1, 1, 128)     147584      conv5[0][0]                      
____________________________________________________________________________________________________
conv6 (Convolution2D)            (None, 1, 1, 256)     295168      conv5_5[0][0]                    
____________________________________________________________________________________________________
conv7 (Convolution2D)            (None, 1, 1, 256)     590080      conv6[0][0]                      
____________________________________________________________________________________________________
drop6 (Dropout)                  (None, 1, 1, 256)     0           conv7[0][0]                      
____________________________________________________________________________________________________
flatten1 (Flatten)               (None, 256)           0           drop6[0][0]                      
____________________________________________________________________________________________________
dense2 (Dense)                   (None, 512)           131584      flatten1[0][0]                   
____________________________________________________________________________________________________
drop5 (Dropout)                  (None, 512)           0           dense2[0][0]                     
____________________________________________________________________________________________________
dense_3 (Dense)                  (None, 1)             513         drop5[0][0]                      
====================================================================================================
Total params: 1,415,785
Trainable params: 1,415,785





I finally randomly shuffled the data set and put 10-5% of the data into a validation set.

I used this training data for training the model. The validation set helped determine if the model was over or under fitting. The ideal number of epochs was 2-3 with data size in keras fit generator being 3x of training data, given to it. 
